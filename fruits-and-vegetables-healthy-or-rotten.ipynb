{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8463025,"sourceType":"datasetVersion","datasetId":5045111}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nIn recent years, the application of computer vision in agriculture has gained significant traction due to its potential to revolutionize farming practices. One critical area where computer vision can make a substantial impact is in the classification of fruits and vegetables based on their health status. This project aims to develop a robust computer vision model using Keras, KerasCV, and TensorFlow to classify images of fruits and vegetables as either healthy or rotten.\n\nThe classification of fruits and vegetables is crucial for various stakeholders in the agricultural supply chain, including farmers, retailers, and consumers. For farmers, early detection of rotten produce can help in reducing waste and optimizing harvesting practices. For retailers, accurate classification can enhance inventory management and ensure that only high-quality products reach the shelves. For consumers, it provides assurance of the quality and safety of the produce they purchase.","metadata":{}},{"cell_type":"markdown","source":"# Problem Statement\nThe primary objective of this project is to develop an efficient and accurate image classification model that can differentiate between rotten and healthy fruits and vegetables. The ability to automatically detect and classify the condition of produce is crucial for several reasons:\n1. **Quality Control**: Ensuring that only high-quality, healthy produce reaches the market, thereby maintaining consumer trust and satisfaction.\n2. **Resource Optimization**: Reducing waste by identifying and removing rotten produce early in the supply chain, which can lead to significant cost savings.\n3. **Food Safety**: Preventing the distribution of rotten produce, which can pose health risks to consumers.\n\nTo achieve this, we will employ a dataset of approximately 30,000 images, covering a wide range of fruits and vegetables in both healthy and rotten states. The project will involve the following key steps:\n1. **Image Preprocessing**: Enhancing the quality and consistency of the input images to improve model performance.\n2. **Image Augmentation**: Expanding the dataset by applying various transformations to the images, which helps in improving the model's generalization capabilities.\n3. **Model Fine-Tuning**: Utilizing the EfficientNetV2B3 architecture, a state-of-the-art convolutional neural network, and fine-tuning it on our specific dataset to achieve high accuracy in classification.","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\nimport shutil\nimport keras\nimport random\nimport keras_cv\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport seaborn as sns\nimport tensorflow as tf\nimport keras.backend as K\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom keras.utils import image_dataset_from_directory\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:09:33.4927Z","iopub.execute_input":"2024-09-09T06:09:33.49313Z","iopub.status.idle":"2024-09-09T06:09:55.204124Z","shell.execute_reply.started":"2024-09-09T06:09:33.493075Z","shell.execute_reply":"2024-09-09T06:09:55.202934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:09:55.20574Z","iopub.execute_input":"2024-09-09T06:09:55.206325Z","iopub.status.idle":"2024-09-09T06:09:55.211758Z","shell.execute_reply.started":"2024-09-09T06:09:55.206287Z","shell.execute_reply":"2024-09-09T06:09:55.210384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reproducability\ntf.keras.utils.set_random_seed(101)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring the working directory","metadata":{}},{"cell_type":"code","source":"def get_sub_dirs(directory):\n    sub_dirs = []\n    for root, _, _, in os.walk(directory):\n        if root.split('/')[-1].split('__')[-1] in ['Rotten', 'Healthy']:\n            sub_dirs.append(root)\n    sub_dirs.sort()\n    return sub_dirs","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:10:37.372184Z","iopub.execute_input":"2024-09-09T06:10:37.372625Z","iopub.status.idle":"2024-09-09T06:10:37.379123Z","shell.execute_reply.started":"2024-09-09T06:10:37.372579Z","shell.execute_reply":"2024-09-09T06:10:37.377617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a list of all sub-directories pertaining to image categories in /kaggle/input/\nsub_dirs = get_sub_dirs('/kaggle/input/')\nsub_dirs[:5]","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:10:37.642647Z","iopub.execute_input":"2024-09-09T06:10:37.643061Z","iopub.status.idle":"2024-09-09T06:11:10.276092Z","shell.execute_reply.started":"2024-09-09T06:10:37.643027Z","shell.execute_reply":"2024-09-09T06:11:10.274675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the total number of images\ntotal_images = 0\nfor path in sub_dirs:\n    total_images += len(os.listdir(path))\ntotal_images","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:13:53.451673Z","iopub.execute_input":"2024-09-09T06:13:53.452067Z","iopub.status.idle":"2024-09-09T06:13:53.505239Z","shell.execute_reply.started":"2024-09-09T06:13:53.452033Z","shell.execute_reply":"2024-09-09T06:13:53.504144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the directory paths\ntrain_dir = '/kaggle/working/training-images/'\nvalid_dir = '/kaggle/working/validation-images/'\ntest_dir = '/kaggle/working/test-images/'","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:14:00.638778Z","iopub.execute_input":"2024-09-09T06:14:00.639193Z","iopub.status.idle":"2024-09-09T06:14:00.644666Z","shell.execute_reply.started":"2024-09-09T06:14:00.639156Z","shell.execute_reply":"2024-09-09T06:14:00.643265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the sub directories\nfor x in sub_dirs:\n    os.makedirs(train_dir + x.split('/')[-1], exist_ok=True)\n    os.makedirs(valid_dir + x.split('/')[-1], exist_ok=True)\n    os.makedirs(test_dir + x.split('/')[-1], exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:14:06.443952Z","iopub.execute_input":"2024-09-09T06:14:06.444396Z","iopub.status.idle":"2024-09-09T06:14:06.455919Z","shell.execute_reply.started":"2024-09-09T06:14:06.444348Z","shell.execute_reply":"2024-09-09T06:14:06.454496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"* Let's create a dataframe for all types of fruits and vegetables images","metadata":{}},{"cell_type":"code","source":"fruits_and_veggies = []\nnum_total = []\nnum_rotten = []\nnum_healthy = []\n\n\nfor path in sub_dirs:\n    x = path.split('/')[-1].split('__')[0]\n    if x not in fruits_and_veggies:\n        fruits_and_veggies.append(x)\n\n\nfor x in fruits_and_veggies:\n    num = 0\n    rotten = 0\n    healthy = 0\n    for dirs in sub_dirs:\n        if x in dirs:\n            num += len(os.listdir(dirs))\n        if f\"{x}__Rotten\" in dirs:\n            rotten += len(os.listdir(dirs))\n        if f\"{x}__Healthy\" in dirs:\n            healthy += len(os.listdir(dirs))\n    num_total.append(num)\n    num_rotten.append(rotten)\n    num_healthy.append(healthy)\n\n\nfruits_and_veggies_dict = {\"fruits\" : fruits_and_veggies, \"total\" : num_total, \"rotten\" : num_rotten, \"healthy\": num_healthy}\nfruits_and_veggies_df = pd.DataFrame.from_dict(fruits_and_veggies_dict).sort_values(by=['total'])","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:14:39.178355Z","iopub.execute_input":"2024-09-09T06:14:39.178777Z","iopub.status.idle":"2024-09-09T06:14:39.266073Z","shell.execute_reply.started":"2024-09-09T06:14:39.178738Z","shell.execute_reply":"2024-09-09T06:14:39.264957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6), dpi=150)\nplt.title(\"Number of total fruits/vegetables\")\nplt.xticks(rotation=90)\nax = sns.barplot(fruits_and_veggies_df, x='fruits', y='total')\nax.bar_label(ax.containers[0], fontsize=10);","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:14:44.949126Z","iopub.execute_input":"2024-09-09T06:14:44.95028Z","iopub.status.idle":"2024-09-09T06:14:45.564049Z","shell.execute_reply.started":"2024-09-09T06:14:44.95023Z","shell.execute_reply":"2024-09-09T06:14:45.562662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6), dpi=150)\nplt.title(\"Number of rotten fruits/vegetables\")\nplt.xticks(rotation=90)\nax = sns.barplot(fruits_and_veggies_df, x='fruits', y='rotten')\nax.bar_label(ax.containers[0], fontsize=10);","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:14:53.981429Z","iopub.execute_input":"2024-09-09T06:14:53.981884Z","iopub.status.idle":"2024-09-09T06:14:54.500135Z","shell.execute_reply.started":"2024-09-09T06:14:53.98184Z","shell.execute_reply":"2024-09-09T06:14:54.498949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6), dpi=150)\nplt.title(\"Number of healthy fruits/vegetables\")\nplt.xticks(rotation=90)\nax = sns.barplot(fruits_and_veggies_df, x='fruits', y='healthy')\nax.bar_label(ax.containers[0], fontsize=10);","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:14:59.521238Z","iopub.execute_input":"2024-09-09T06:14:59.521806Z","iopub.status.idle":"2024-09-09T06:15:00.057257Z","shell.execute_reply.started":"2024-09-09T06:14:59.521756Z","shell.execute_reply":"2024-09-09T06:15:00.055883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for plotting 2 random images from a folder\ndef plot_random_images_from_directory(directory_path):\n    all_files = os.listdir(directory_path) \n    if len(all_files) < 4:\n        print(\"Not enough images in the directory to display.\")\n        return\n    selected_images = random.sample(all_files, 2)\n    fig, axes = plt.subplots(1, 2, figsize=(4, 4), dpi=100)\n    for i, ax in enumerate(axes.flat):\n        image_path = os.path.join(directory_path, selected_images[i])\n        img = mpimg.imread(image_path)\n        ax.imshow(img)\n        ax.axis('off')\n    class_name = os.path.basename(directory_path)\n    fig.suptitle(class_name, fontsize=16)\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:15:07.449995Z","iopub.execute_input":"2024-09-09T06:15:07.450409Z","iopub.status.idle":"2024-09-09T06:15:07.459732Z","shell.execute_reply.started":"2024-09-09T06:15:07.450369Z","shell.execute_reply":"2024-09-09T06:15:07.458266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting 2 random images of each type of rotten and healthy fruits and vegetables\nfor directory in sub_dirs:\n    plot_random_images_from_directory(directory)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:15:15.791179Z","iopub.execute_input":"2024-09-09T06:15:15.791626Z","iopub.status.idle":"2024-09-09T06:15:33.507068Z","shell.execute_reply.started":"2024-09-09T06:15:15.791584Z","shell.execute_reply":"2024-09-09T06:15:33.505552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Copying the images","metadata":{}},{"cell_type":"markdown","source":"* We will now copy all the images to the training-images directory\n* From there, we will move 15 images of each type of fruits and vegetables (both rotten and healthy) to the validation-images and test-images directories","metadata":{}},{"cell_type":"code","source":"# Getting the list of all subdirectories in that training-images directory\ntrain_sub_dirs = get_sub_dirs(train_dir)\ntrain_sub_dirs[:5]","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:15:54.838608Z","iopub.execute_input":"2024-09-09T06:15:54.839795Z","iopub.status.idle":"2024-09-09T06:15:54.850418Z","shell.execute_reply.started":"2024-09-09T06:15:54.839674Z","shell.execute_reply":"2024-09-09T06:15:54.849293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We will use multi-threading to copy the images faster**","metadata":{}},{"cell_type":"code","source":"def copy_file(src, dst):\n    shutil.copy(src, dst)\n\ndef copy_files(source_dirs, dest_dir):\n    global train_sub_dirs\n    num_files = 0\n    tasks = []\n\n    # Create destination directories if they don't exist\n    for path in source_dirs:\n        dst = os.path.join(dest_dir, os.path.basename(path))\n        os.makedirs(dst, exist_ok=True)\n        for filename in os.listdir(path):\n            num_files += 1\n            src = os.path.join(path, filename)\n            tasks.append((src, dst))\n\n    # Use ThreadPoolExecutor to copy files\n    with ThreadPoolExecutor() as executor:\n        executor.map(lambda task: copy_file(*task), tasks)\n\n    count = 0\n    for i in range(len(train_sub_dirs)):\n        source_num = len(os.listdir(source_dirs[i]))\n        dest_num = len(os.listdir(train_sub_dirs[i]))\n        if source_num != dest_num:\n            count += 1\n            print(f\"{source_num - dest_num} files were not copied to {train_sub_dirs[i]}\")\n\n    if count == 0:\n        print(f\"All {num_files} files are copied to the training directory.\")\n\n# Example usage\nif __name__ == \"__main__\":\n    copy_files(sub_dirs, train_dir)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:16:13.354073Z","iopub.execute_input":"2024-09-09T06:16:13.35497Z","iopub.status.idle":"2024-09-09T06:16:51.020879Z","shell.execute_reply.started":"2024-09-09T06:16:13.354921Z","shell.execute_reply":"2024-09-09T06:16:51.019508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Preprocessing","metadata":{}},{"cell_type":"markdown","source":"* Let's create a function that will return a dictionary with keys as the image filetype, and values as the number of images with that filetype","metadata":{}},{"cell_type":"code","source":"def get_img_extensions_dict(dir_paths):\n    img_extension_dict = {}\n    for path in dir_paths:\n        for filename in os.listdir(path):\n            _, file_extension = os.path.splitext(filename)\n            if file_extension not in img_extension_dict:\n                img_extension_dict[file_extension] = 1\n            else:\n                img_extension_dict[file_extension] += 1\n    return img_extension_dict","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:18:36.353462Z","iopub.execute_input":"2024-09-09T06:18:36.355347Z","iopub.status.idle":"2024-09-09T06:18:36.36288Z","shell.execute_reply.started":"2024-09-09T06:18:36.355285Z","shell.execute_reply":"2024-09-09T06:18:36.361392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(get_img_extensions_dict(train_sub_dirs))","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:18:42.170887Z","iopub.execute_input":"2024-09-09T06:18:42.171312Z","iopub.status.idle":"2024-09-09T06:18:42.247003Z","shell.execute_reply.started":"2024-09-09T06:18:42.171271Z","shell.execute_reply":"2024-09-09T06:18:42.245636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We can see that we have images with different filetypes.\n* This means that the images might also have different numbers of color channels as well.\n* So, let's create a function that will iterate through our training directory and will return a dictionary where the keys will be the color chanels, and the values will be the total number of images with that color channel.","metadata":{}},{"cell_type":"code","source":"def count_color_channels(folder_paths):\n    channel_counts = {}\n    mode_to_channels = {'L': 1, 'LA': 2, 'P': 1, 'RGB': 3, 'RGBA': 4, 'CMYK': 4, 'YCbCr': 3, 'HSV': 3, 'LAB': 3, 'I': 1, 'F': 1}\n    for folder_path in folder_paths:\n        for file_name in os.listdir(folder_path):\n            file_path = os.path.join(folder_path, file_name)\n            try:\n                with Image.open(file_path) as img:\n                    mode = img.mode\n                    channels = mode_to_channels.get(mode)\n                    if channels is not None:\n                        if channels in channel_counts:\n                            channel_counts[channels] += 1\n                        else:\n                            channel_counts[channels] = 1\n                    else:\n                        print(f\"Unrecognized mode {mode} for file {file_path}\")\n            except Exception as e:\n                print(f\"Error processing file {file_path}: {e}\")\n    \n    return channel_counts","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:20:01.741194Z","iopub.execute_input":"2024-09-09T06:20:01.742471Z","iopub.status.idle":"2024-09-09T06:20:01.752085Z","shell.execute_reply.started":"2024-09-09T06:20:01.742411Z","shell.execute_reply":"2024-09-09T06:20:01.750588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_channel_counts = count_color_channels(train_sub_dirs)\nprint(color_channel_counts)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:20:07.129808Z","iopub.execute_input":"2024-09-09T06:20:07.130226Z","iopub.status.idle":"2024-09-09T06:20:27.101552Z","shell.execute_reply.started":"2024-09-09T06:20:07.130189Z","shell.execute_reply":"2024-09-09T06:20:27.100276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Since we have images with different number of color channels, this will create a problem later while batching and modelling.\n* So, let's create a function that will iterate overe all the images in the training directory and convert the images to have the JPEG filetype and RGB as the color channel.\n* **NOTE:** We will use multiprocessing to speed up this task.","metadata":{}},{"cell_type":"code","source":"def convert_images_to_rgb_and_jpeg(directory):\n    for filename in os.listdir(directory):\n        if filename.lower().endswith(('.jpg', '.png', '.webp', '.jpeg', '.JPG')):\n            file_path = os.path.join(directory, filename)\n            new_file_path = os.path.join(directory, os.path.splitext(filename)[0] + '.jpg')\n\n            with Image.open(file_path) as img:\n                if img.mode != 'RGB':\n                    img = img.convert('RGB')\n                img.save(new_file_path, 'JPEG')\n\n            # Remove the original file if it's not already a JPEG\n            if not filename.lower().endswith('.jpg'):\n                os.remove(file_path)\n\n\ndef process_folder(folder_path):\n    convert_images_to_rgb_and_jpeg(folder_path)\n    print(f'Processed folder: {folder_path}')\n\nwith ProcessPoolExecutor() as executor:\n    executor.map(process_folder, train_sub_dirs)\n\nprint('All the images were converted to JPEG format and to have the RGB color channel.')","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:20:33.136242Z","iopub.execute_input":"2024-09-09T06:20:33.136695Z","iopub.status.idle":"2024-09-09T06:23:02.12468Z","shell.execute_reply.started":"2024-09-09T06:20:33.136655Z","shell.execute_reply":"2024-09-09T06:23:02.12292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Let's now again check the number of each image filetypes and color channels","metadata":{}},{"cell_type":"code","source":"print(get_img_extensions_dict(train_sub_dirs))","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:24:11.862719Z","iopub.execute_input":"2024-09-09T06:24:11.863409Z","iopub.status.idle":"2024-09-09T06:24:11.948823Z","shell.execute_reply.started":"2024-09-09T06:24:11.863363Z","shell.execute_reply":"2024-09-09T06:24:11.947613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_channel_counts = count_color_channels(train_sub_dirs)\nprint(color_channel_counts)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:24:19.719586Z","iopub.execute_input":"2024-09-09T06:24:19.720129Z","iopub.status.idle":"2024-09-09T06:24:22.521653Z","shell.execute_reply.started":"2024-09-09T06:24:19.720079Z","shell.execute_reply":"2024-09-09T06:24:22.520464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We can see that all the images were converted successfully","metadata":{}},{"cell_type":"markdown","source":"### Moving the images to the validation and test directories","metadata":{}},{"cell_type":"markdown","source":"* We will now move 15 images from each category of rotten and healthy fruits and vegetables to the validation and test directories","metadata":{}},{"cell_type":"code","source":"for root, _, _ in os.walk('/kaggle/working/training-images'):\n    if root.split('/')[-1] != 'training-images':\n        for i in range(15):\n            src_path_test = os.path.join(root, random.choice(os.listdir(root)))\n            dst_path_test = '/kaggle/working/test-images/' + root.split('/')[-1]\n            shutil.move(src_path_test, dst_path_test)\n            src_path_valid = os.path.join(root, random.choice(os.listdir(root)))\n            dst_path_valid = '/kaggle/working/validation-images/' + root.split('/')[-1]\n            shutil.move(src_path_valid, dst_path_valid)\n\nprint(\"15 images of each types of rotten and healthy fruits and vegetables were moved to the validation and test directories.\")","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:25:47.710418Z","iopub.execute_input":"2024-09-09T06:25:47.712188Z","iopub.status.idle":"2024-09-09T06:25:48.36446Z","shell.execute_reply.started":"2024-09-09T06:25:47.712122Z","shell.execute_reply":"2024-09-09T06:25:48.362998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building tensorflow datasets","metadata":{}},{"cell_type":"code","source":"def build_img_dataset(directory, BATCH_SIZE=32, SEED=101, IMG_SIZE=(224, 224), class_names=False, shuffle=True):\n    dataset = keras.preprocessing.image_dataset_from_directory(\n        directory=directory,\n        batch_size=BATCH_SIZE,\n        label_mode='categorical',\n        image_size=IMG_SIZE,\n        interpolation='bilinear',\n        shuffle=shuffle,\n        seed=SEED\n    )\n    \n    class_names = dataset.class_names\n    \n    dataset = dataset.cache().prefetch(tf.data.AUTOTUNE)\n    \n    return (dataset, class_names if class_names else None)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:26:07.798388Z","iopub.execute_input":"2024-09-09T06:26:07.79953Z","iopub.status.idle":"2024-09-09T06:26:07.808508Z","shell.execute_reply.started":"2024-09-09T06:26:07.799469Z","shell.execute_reply":"2024-09-09T06:26:07.806871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = build_img_dataset(train_dir)[0]\nvalid_data = build_img_dataset(valid_dir)[0]\ntest_data, test_classes = build_img_dataset(test_dir, BATCH_SIZE=16, class_names=True) # will need test_classes in visualizing predictions","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:26:59.95431Z","iopub.execute_input":"2024-09-09T06:26:59.954784Z","iopub.status.idle":"2024-09-09T06:27:02.338982Z","shell.execute_reply.started":"2024-09-09T06:26:59.95474Z","shell.execute_reply":"2024-09-09T06:27:02.337709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Augmentation","metadata":{}},{"cell_type":"markdown","source":"Data augmentation is a technique to make your model robust to changes in input data such as lighting, cropping, and orientation. One caveat to be aware of with image data augmentation is that you must be careful to not shift your augmented data distribution too far from the original data distribution. The goal is to prevent overfitting and increase generalization, but samples that lie completely out of the data distribution simply add noise to the training process.\n\n\n[[source]](https://keras.io/guides/keras_cv/classification_with_keras_cv/)","metadata":{}},{"cell_type":"code","source":"def pack_to_dict(image, label):\n    return {\"images\": image, \"labels\": label}\n\ntrain_data = train_data.map(pack_to_dict, num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:28:23.104047Z","iopub.execute_input":"2024-09-09T06:28:23.10505Z","iopub.status.idle":"2024-09-09T06:28:23.21231Z","shell.execute_reply.started":"2024-09-09T06:28:23.104991Z","shell.execute_reply":"2024-09-09T06:28:23.210866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(train_data.take(1)))\nimage_batch = batch[\"images\"]\nlabel_batch = batch[\"labels\"]\n\ndef visualize_images(rows=2, cols=2, augmentation=None, images=image_batch):\n    keras_cv.visualization.plot_image_gallery(\n        augmentation(image_batch) if augmentation else image_batch,\n        rows=rows,\n        cols=cols,\n        value_range=(0, 255),\n        show=True,\n    )","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:28:29.516807Z","iopub.execute_input":"2024-09-09T06:28:29.517257Z","iopub.status.idle":"2024-09-09T06:28:29.842328Z","shell.execute_reply.started":"2024-09-09T06:28:29.517215Z","shell.execute_reply":"2024-09-09T06:28:29.840795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's visualize some images before applying augmentation\nvisualize_images()","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:29:10.676856Z","iopub.execute_input":"2024-09-09T06:29:10.678213Z","iopub.status.idle":"2024-09-09T06:29:11.026279Z","shell.execute_reply.started":"2024-09-09T06:29:10.678152Z","shell.execute_reply":"2024-09-09T06:29:11.024905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RandomFlip Augmentation","metadata":{}},{"cell_type":"code","source":"random_flip = keras_cv.layers.RandomFlip(\n    mode=\"horizontal_and_vertical\"\n)\nvisualize_images(augmentation = random_flip)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:31:32.121549Z","iopub.execute_input":"2024-09-09T06:31:32.122117Z","iopub.status.idle":"2024-09-09T06:31:33.471255Z","shell.execute_reply.started":"2024-09-09T06:31:32.12207Z","shell.execute_reply":"2024-09-09T06:31:33.469695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RandomCropAndResize Augmentation","metadata":{}},{"cell_type":"markdown","source":"The next augmentation we'll use is RandomCropAndResize. This operation selects a random subset of the image, then resizes it to the provided target size. By using this augmentation, we force our classifier to become spatially invariant. Additionally, this layer accepts an ```aspect_ratio_factor``` which can be used to distort the aspect ratio of the image. While this can improve model performance, it should be used with caution. It is very easy for an aspect ratio distortion to shift a sample too far from the original training set's data distribution. \n\n\n[[source]](https://keras.io/guides/keras_cv/classification_with_keras_cv/)","metadata":{}},{"cell_type":"code","source":"random_crop_and_resize = keras_cv.layers.RandomCropAndResize(\n    target_size = (224, 224),\n    crop_area_factor = (0.8, 1.0),\n    aspect_ratio_factor = (0.9, 1.1),\n)\n\nvisualize_images(augmentation = random_crop_and_resize)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:32:06.298149Z","iopub.execute_input":"2024-09-09T06:32:06.298631Z","iopub.status.idle":"2024-09-09T06:32:07.532983Z","shell.execute_reply.started":"2024-09-09T06:32:06.298588Z","shell.execute_reply":"2024-09-09T06:32:07.531694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RandomShear Augmentation","metadata":{}},{"cell_type":"code","source":"random_shear = keras_cv.layers.RandomShear(\n    x_factor = (0.2, 0.4),\n    y_factor = (0.2, 0.4),\n    interpolation = \"bilinear\",\n    fill_mode = \"reflect\",\n)\n\nvisualize_images(augmentation = random_shear)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:32:38.122521Z","iopub.execute_input":"2024-09-09T06:32:38.12301Z","iopub.status.idle":"2024-09-09T06:32:38.59604Z","shell.execute_reply.started":"2024-09-09T06:32:38.122966Z","shell.execute_reply":"2024-09-09T06:32:38.594778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RandomHue Augmentation","metadata":{}},{"cell_type":"code","source":"random_hue = keras_cv.layers.RandomHue(\n    factor = (0.2, 0.3),\n    value_range = [0, 255]\n)\n\nvisualize_images(augmentation = random_hue)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:33:13.22131Z","iopub.execute_input":"2024-09-09T06:33:13.221805Z","iopub.status.idle":"2024-09-09T06:33:13.737153Z","shell.execute_reply.started":"2024-09-09T06:33:13.221764Z","shell.execute_reply":"2024-09-09T06:33:13.735901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building image augmentation pipeline","metadata":{}},{"cell_type":"code","source":"aug_pipeline = keras_cv.layers.RandomAugmentationPipeline(\n    layers=[random_flip, random_crop_and_resize, random_hue, random_shear],\n    augmentations_per_image=4,\n)\n\ntrain_data = train_data.map(aug_pipeline, num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:33:54.086631Z","iopub.execute_input":"2024-09-09T06:33:54.087095Z","iopub.status.idle":"2024-09-09T06:33:56.030132Z","shell.execute_reply.started":"2024-09-09T06:33:54.087054Z","shell.execute_reply":"2024-09-09T06:33:56.028666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing some images after applying the augmentation pipeline","metadata":{}},{"cell_type":"code","source":"image_batch = next(iter(train_data.take(1)))['images']\nvisualize_images(images = image_batch)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:34:22.401263Z","iopub.execute_input":"2024-09-09T06:34:22.401743Z","iopub.status.idle":"2024-09-09T06:34:25.064294Z","shell.execute_reply.started":"2024-09-09T06:34:22.4017Z","shell.execute_reply":"2024-09-09T06:34:25.062773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unpackage_dict(inputs):\n    return inputs[\"images\"], inputs[\"labels\"]\n\ntrain_data = train_data.map(unpackage_dict, num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:34:32.093479Z","iopub.execute_input":"2024-09-09T06:34:32.094034Z","iopub.status.idle":"2024-09-09T06:34:32.150276Z","shell.execute_reply.started":"2024-09-09T06:34:32.093985Z","shell.execute_reply":"2024-09-09T06:34:32.148583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"code","source":"base_model = tf.keras.applications.EfficientNetV2B0(\n    include_top=False,\n    input_shape=(224, 224, 3),\n    pooling='avg',\n    include_preprocessing=True\n)\n\ninputs = tf.keras.layers.Input(shape=(None, None, 3))\nresized_inputs = tf.keras.layers.Resizing(224, 224)(inputs)\nx = base_model(resized_inputs)\noutputs = tf.keras.layers.Dense(len(test_classes), activation='softmax')(x)\nmodel = tf.keras.Model(inputs, outputs)\n\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=tf.keras.optimizers.AdamW(1e-4),\n    metrics=['accuracy']\n)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-09-09T06:34:56.080427Z","iopub.execute_input":"2024-09-09T06:34:56.081807Z","iopub.status.idle":"2024-09-09T06:35:10.668922Z","shell.execute_reply.started":"2024-09-09T06:34:56.081741Z","shell.execute_reply":"2024-09-09T06:35:10.667659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    train_data,\n    validation_data=valid_data,\n    epochs=5,\n    verbose=1,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting the model accuracy and model loss","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 4), dpi=200)\n# Plot loss\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='train_loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Loss over Epochs')\n\n# Plot accuracy\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('accuracy')\nplt.legend()\nplt.title('Accuracy over Epochs')\n\nplt.show();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"K.clear_session()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = model.evaluate(test_data)\nprint(f\"The model loss on the test dataset is {round(loss, 4)}.\")\nprint(f\"The model accuracy on the test dataset is {round(accuracy, 2)*100}%.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We will now plot 9 random images from the test dataset along with their true and predcited labels\n* If the predicted label matches the original label, the labels will be coloured in green, else, the labels will be coloured in red","metadata":{}},{"cell_type":"code","source":"def display_images_with_predictions_and_true_labels(dataset, model, test_classes, num_images=9):\n    plt.figure(figsize=(10, 10))\n\n    # Take a batch from the dataset and select random images\n    for images, labels in dataset.take(1):\n        indices = np.random.choice(images.shape[0], num_images, replace=False)\n        random_images = images.numpy()[indices]\n        true_labels = labels.numpy()[indices]\n        \n        # Convert true labels to integer indices if they are one-hot encoded\n        if len(true_labels.shape) > 1:\n            true_labels = np.argmax(true_labels, axis=1)\n\n        predictions = model.predict(random_images)\n        predicted_labels = np.argmax(predictions, axis=1)\n\n        for i in range(num_images):\n            plt.subplot(3, 3, i + 1)\n            plt.imshow(random_images[i].astype(\"uint8\"))\n            if true_labels[i] == predicted_labels[i]:\n                color = 'green'\n            else:\n                color = 'red'\n            plt.title(f\"True: {test_classes[true_labels[i]]}\\nPredicted: {test_classes[predicted_labels[i]]}\",\n                      color=color)\n            plt.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_images_with_predictions_and_true_labels(test_data, model, test_classes)","metadata":{},"execution_count":null,"outputs":[]}]}